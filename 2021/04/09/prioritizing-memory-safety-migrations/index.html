<!doctype html><html lang=en-us><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=author content="Chris Palmer"><meta name=robots content="noai, noimageai"><title>Prioritizing Memory Safety Migrations</title><link rel=alternate type=application/rss+xml href=/feed/ title=Noncombatant><link rel=icon href="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAxMDAgMTAwIj48dGV4dCB5PSIuOWVtIiBmb250LXNpemU9IjkwIj7wn462PC90ZXh0Pjwvc3ZnPg=="><style>html{-moz-text-size-adjust:none;-webkit-text-size-adjust:none;text-size-adjust:none}body{line-height:1.4;font-family:helvetica neue,Helvetica,Roboto,Arial,sans-serif;text-wrap:pretty;font-variant-numeric:oldstyle-nums proportional-nums;color:#222;background-color:#fff}@media(min-width:55em){body{margin-inline-start:5em}}h1,h2,h3,h4,h5{line-height:1.2}h1,h2,h3,h4,h5,p,ul,ol,dl,fieldset,aside,figcaption,nav,header,footer{max-width:40rem}blockquote{max-width:35rem}dt{font-weight:700}pre,code,kbd{font-family:Inconsolata,Monaco,Consolas,monospace}pre{white-space:pre-wrap}footer,nav,aside,figcaption{opacity:.8;font-size:90%}footer,nav{margin-top:3em}img{border:1px solid #222;border-radius:3px;max-width:100%;height:auto}input,button,textarea,select{font-size:inherit;font-family:inherit}table{border-spacing:0}td,th{text-align:left;vertical-align:bottom;padding:.25em}td,math,time[datetime*=":"]{font-variant-numeric:tabular-nums lining-nums slashed-zero}table tr:nth-child(even){background-color:#eee}.right{text-align:right}.bottom{vertical-align:bottom}@view-transition{navigation:auto;}::view-transition-group(root){animation-duration:.5s}</style><nav><a href=/>Noncombatant</a>
üôÇ&nbsp;<a href=/about/>About</a>
‚úçÔ∏è&nbsp;<a href=/publications/>Other Writing</a>
üéµ&nbsp;<a href=https://noncombatant.bandcamp.com/>Bandcamp</a>
üíª&nbsp;<a href=https://github.com/noncombatant>GitHub</a>
üêò&nbsp;<a rel=me href=https://wandering.shop/@fugueish>Mastodon</a></nav><style>img{border:0}</style><h1>Prioritizing Memory Safety Migrations</h1><p><time>9 April 2021</time><aside><p>Update 11 April: Please also see <a href=/2021/04/11/long-live-sandboxing/>Long Live Sandboxing!</a>. Sandboxing
is not dead, despite what you might have heard.</aside><p>With all the talk of using Rust to reduce memory unsafety bugs, <a href=https://security.googleblog.com/2021/04/rust-in-android-platform.html>such
as Android using Rust in the Android Open Source Project</a>, there‚Äôs a lot of
extremely reasonable concern about the high cost of ‚Äúrewriting it all in Rust‚Äù
(or any other safer language), as it‚Äôs often phrased. Operating systems, web
browsers, complex online services, and so on can be implemented with tens of
millions of lines of C/C++ code. (<a href=https://www.technologyreview.com/2012/12/03/181350/many-cars-have-a-hundred-million-lines-of-code/>Sometimes
more</a>.) Rewriting all that seems prohibitively expensive, and exacerbates <a href=https://www.usenix.org/conference/enigma2021/presentation/gaynor>what
Alex Gaynor aptly calls grief</a> ‚Äî¬†people stay in the denial stage longer when
struck by the enormity of the memory unsafety problem.<p>Thankfully, <a href=https://wiki.mozilla.org/Oxidation>replacing C/C++ with
code in a safer language</a> is not an all-or-nothing task. We can do it
gradually; some parts we might never need to replace. Most safer languages can
link in the same address space as C and/or C++, and call into and be called by
C/C++. You can also normalize data structures such that the safe code handles
arbitrary inputs, and the C/C++ code can focus on a single, simpler grammar. For
example:<figure><img src=formats.png width=328 height=347 alt="internet ‚Üí { PNG,
JPEG, GIF, TIFF, ... } ‚Üí SkBitmap" loading=lazy><figcaption>You can accept
arbitrary image (e.g.) formats from the internet, use a safer language to
normalize them into Skia‚Äôs simple <code>SkBitmap</code> format, and then handle
the bitmaps in Skia in C++. This simplifies the C++ code (reducing its attack
surface), and provides a simple cross-language interface.</figcaption></figure><p>But how do you tell where to start replacing C/C++ with safer code, and where
to stop?<p>Although security is certainly not the only benefit of a safe language ‚Äî¬†the
Android team‚Äôs post starts out stressing correctness ‚Äî my perspective is
security. And from that starting point, we can use what I think is a pretty
clear method to prioritize our efforts.<p>Even if you have, say, 20 million lines of C++ code, not all of it is
directly or indirectly exposed to attackers. You can start hardening the most
exposed code first, and you can rank exposure by how long the path to the code
is. Consider <a href=https://googleprojectzero.blogspot.com/2020/12/an-ios-zero-click-radio-proximity.html>Ian
Beer‚Äôs epic radio pyrotechnics</a>, in which he compromised iPhones by sending
them mean-spirited packets by radio. We can model the attack surface exposure
something like this:<figure><img src=surface-01.png width=356 height=59 alt="attacker ‚Üí radio
chip ‚Üí kernel" loading=lazy><figcaption>An attack pathway from the internet
to the kernel.</figcaption></figure><p>That‚Äôs a bit of an oversimplification, but it lets us see that the attacker‚Äôs
call graph is not very deep ‚Äî¬†that is, that driver is pretty exposed.<p>Additionally, as the title of Ian‚Äôs post points out, the attacker‚Äôs cost to
traverse the first few edges is 0. We can model that by assigning ‚Äòweight‚Äô or
‚Äòcost‚Äô to the edges in the graph. The higher the cost, the less likely it is
that the attacker will succeed. Assuming the radio is fairly simple and does
little normalization or filtering before passing what it got to the kernel, we
might draw something like this:<figure><img src=surface-02.png width=416 height=61 alt="attacker ‚Üí (0)
radio chip ‚Üí (low or medium) kernel" loading=lazy><figcaption>An attack
pathway from the internet to the kernel, with estimated costs for each edge
traversal.</figcaption></figure><p>On a scale from 0 &lt; low &lt; medium &lt; high, we might generously
estimate the cost to exploit the vulnerable driver to be maybe medium. If the
defender is lucky, maybe ASLR is working, or something.<p>Ian explains everything in full detail in his post, but in general we should
not think of C/C++ code as defensible. If an attacker is able to get at C/C++
attack surface, we must assume they can win with an exploit based on memory
unsafety.<p>As an additional example, consider your web server‚Äôs or browser‚Äôs TLS
implementation. Should we consider it exposed? We can model it something like
this:<figure><img src=surface-03.png width=739 height=61 alt="attacker ‚Üí (0,
low, or medium) net interface ‚Üí (passthru) kernel ‚Üí (passthru) TCP ‚Üí (low or
medium) TLS" loading=lazy><figcaption>An attack pathway from the internet to
the application‚Äôs TLS implementation.</figcaption></figure><p>In this case, the attacker is interested only in the application‚Äôs TLS
implementation, and is just using the kernel as a way to get there ‚Äî¬†they are
not attacking the device driver or the TCP implementation. (Although those are
also exposed attack surfaces, of course.) The kernel typically does not do
anything with the application layer traffic, passing it verbatim to the userland
application. So the kernel is not creating a security boundary in this case.<p>The attacker has a pretty straight shot to your application‚Äôs TLS
implementation; the only attack precondition is whether the attacker can send
malicious TLS traffic to the application. Obviously, servers listen to the
internet and process whatever they get; that‚Äôs 0 cost. If attacking a client, an
attacker may have to get the target to contact their server or may have to be on
the same network as the target. We might say that is up to medium cost.<p>So, things like device drivers and HTTP, TCP, and TLS implementations are all
fine candidates for (re)implementing in a safer language. They‚Äôre unavoidably
exposed.<p>Consider an example where the C/C++ attack surface is not as directly
exposed.<figure><img src=surface-04.png width=571 height=59 alt="attacker ‚Üí ... ‚Üí
HTTP ‚Üí parse CSP ‚Üí evaluate CSP" loading=lazy><figcaption>An attack pathway
from the internet to a client‚Äôs CSP evaluator.</figcaption></figure><p>In this example, we have an HTTP client that is going to parse and evaluate a
<a href=https://en.wikipedia.org/wiki/Content_Security_Policy>Content Security
Policy</a> (CSP) header. Each of the network interface, device driver, TCP
implementation, TLS client implementation, HTTP client implementation, and CSP
parser are fairly exposed attack surface. For example, if the attacker wants to
exploit some bug in the CSP parser, they can likely rely on all of the previous
components to pass the header value through verbatim to the CSP parser. Thus,
they probably do not create a security boundary.<p>But if the attacker wants to exploit a likely bug class, mis-evaluation of
CSP policy, they must first get past the CSP parser. Although it is vulnerable
attack surface, it does also provide some security boundary: the policy must be
well-formed according to the grammar the parser accepts. Another bug class is
that the parser‚Äôs grammar is not necessarily the same as the grammar in the
spec.<p>Thus, we‚Äôd be speaking of logic bugs in the CSP parser and/or evaluator. This
is the kind of code that can be buggy in any language; this is not memory
unsafety that can be resolved at scale with a safer language.<p>These examples suggest that you have to get fairly deep into the call graph
before memory unsafety becomes less of a concern. That‚Äôs consistent with <a href=https://alexgaynor.net/2020/may/27/science-on-memory-unsafety-and-security/>the
findings that memory unsafety accounts for anywhere from ‚Öî to ¬æ of
vulnerabilities</a>. The problem is that bad.<p>Models like those above can be step 1 in a process of repair triage. You
might order a set of constraints when filtering through what code to rewrite,
apply mitigations or testing to, or even get rid of first:<ol><li>Select the most exposed code<li>...of that code, start with the highest-privilege code<li>...of that code, start with the code that has the highest observed bug count</ol><p>Or you might triage differently, depending on your situation:<ol><li>Select the most exposed code<li>...of that code, start with the code that has the highest observed bug count<li>...of that code, start with the highest-privilege code</ol><p>Or even:<ol><li>Select the code that has the highest observed bug count<li>...of that code, start with the most exposed code<li>...of that code, start with the highest-privilege code</ol><p>Which approach is appropriate depends on your system. For example, if you are
working entirely in the kernel, all your code runs at the same level of
privilege so you can‚Äôt use that as a filter. Or if you are in userland, but your
application is not making use of process sandboxing, consider exploring that
first before starting a rewrite effort.<p>In any case, we don‚Äôt have to ‚Äúrewrite everything in Rust‚Äù to significantly
improve memory safety, and we are not lost in a sea of undifferentiated attack
surface. There are ways we can prioritize in a somewhat systematic way ‚Äî¬†we
don‚Äôt have to fix random things ad hoc.<p><i>Thanks to Jacob Hoffman-Andrews, Andrew Dunham, and Dev Akhawe for reading
drafts of this post and suggesting helpful improvements!</i><footer><p><a href=https://noncombatant.org/>noncombatant.org</a> by <a xmlns:cc=http://creativecommons.org/ns# href=mailto:chris@noncombatant.org property=cc:attributionName rel=cc:attributionurl>Chris Palmer</a> is
in the
Creative Commons, under the terms of the <a rel=license href=https://creativecommons.org/licenses/by-nc-sa/4.0/>Attribution-NonCommercial-ShareAlike
4.0 International License</a>.</footer>